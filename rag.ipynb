{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05226ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbe9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DHRUV\\AppData\\Local\\Temp\\ipykernel_27864\\3700191053.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model = MODEL)\n",
      "C:\\Users\\DHRUV\\AppData\\Local\\Temp\\ipykernel_27864\\3700191053.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedings = OllamaEmbeddings(model = MODEL)\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"mistral:7b\"\n",
    "model = Ollama(model = MODEL)\n",
    "embedings = OllamaEmbeddings(model = MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da91b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "Answer the question from the content given \n",
    "if you dont know the answer then get reference\n",
    "from your training data\n",
    "\n",
    "Content : {content}\n",
    "Question : {question}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce9bc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-12T02:06:26+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-12T02:06:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'C:\\\\Users\\\\DHRUV\\\\Desktop\\\\IProject\\\\Papers and Refrence Codes\\\\2303.11989v2.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1'}, page_content='Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models\\nLukas H¨ollein1∗ Ang Cao2∗ Andrew Owens2 Justin Johnson2 Matthias Nießner1\\n1Technical University of Munich 2University of Michigan\\n\"a livingroomwithlots ofbookshelves, couches,and smalltables\"\\n… …\\nIterativeGeneration\\n\"a livingroomwithlots ofbookshelves, couches,and smalltables\"\\n… …\\nIterativeGeneration\\n(a) 3D Mesh Generation from Text (b) Rendered Image + Mesh\\nFigure 1. Textured 3D mesh generation from text prompts. We generate textured 3D meshes from a given text prompt using 2D\\ntext-to-image models. (a) The scene is iteratively created from different viewpoints (marked in blue). (b) Our generated mesh contains\\ncompelling textures and geometry. We remove the ceiling in the top-down views for better visualization of the scene layout.\\nAbstract\\nWe present Text2Room†, a method for generating room-\\nscale textured 3D meshes from a given text prompt as input.\\nTo this end, we leverage pre-trained 2D text-to-image mod-\\nels to synthesize a sequence of images from different poses.\\nIn order to lift these outputs into a consistent 3D scene rep-\\nresentation, we combine monocular depth estimation with\\na text-conditioned inpainting model. The core idea of our\\napproach is a tailored viewpoint selection such that the con-\\ntent of each image can be fused into a seamless, textured 3D\\nmesh. More specifically, we propose a continuous align-\\nment strategy that iteratively fuses scene frames with the\\nexisting geometry to create a seamless mesh. Unlike exist-\\ning works that focus on generating single objects [57, 42] or\\nzoom-out trajectories [18] from text, our method generates\\ncomplete 3D scenes with multiple objects and explicit 3D\\ngeometry. We evaluate our approach using qualitative and\\nquantitative metrics, demonstrating it as the first method to\\ngenerate room-scale 3D geometry with compelling textures\\nfrom only text as input.\\n∗ joint first authorship\\n†https://lukashoel.github.io/text-to-room\\n1. Introduction\\nMesh representations of 3D scenes are a crucial com-\\nponent for many applications, from AR/VR asset creation\\nto computer graphics, yet creating these 3D assets remains\\na painstaking process that requires considerable expertise.\\nIn the 2D domain, recent works have successfully cre-\\nated high-quality images from text using generative mod-\\nels, such as diffusion models [66, 59, 68]. These methods\\nsignificantly reduce the barriers to creating images that con-\\ntain a user’s desired content, effectively helping towards the\\ndemocratization of content creation. An emerging line of\\nwork has sought to apply similar methods to create 3D mod-\\nels from text [9, 57, 30, 42, 39], yet existing approaches\\ncome with a number of significant limitations and lack the\\ngenerality of 2D text-to-image models.\\nOne of the core challenges of generating 3D models is\\ncoping with the lack of available 3D training data, as 3D\\ndatasets are vastly smaller than those available in many\\nother applications, such as 2D image synthesis. For ex-\\nample, methods that directly use 3D supervision, such as\\nChen et al . [9], are often limited to datasets of simple\\nshapes, such as ShapeNet [8]. To address these data lim-\\nitations, recent methods [57, 30, 42, 39, 89] lift the expres-\\nsive power of 2D text-to-image models into 3D by formu-\\nlating 3D generation as an iterative optimization problem\\nin the image domain. This allows them to generate 3D ob-\\n1\\narXiv:2303.11989v2  [cs.CV]  10 Sep 2023')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=r\"C:\\Users\\DHRUV\\Desktop\\IProject\\Papers and Refrence Codes\\2303.11989v2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71657093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DHRUV\\anaconda3\\envs\\rag\\lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "vector_store = DocArrayInMemorySearch.from_documents(pages,embedding=embedings)\n",
    "retriver = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db458d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-12T02:06:26+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-12T02:06:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'C:\\\\Users\\\\DHRUV\\\\Desktop\\\\IProject\\\\Papers and Refrence Codes\\\\2303.11989v2.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11'}, page_content='on Computer Vision and Pattern Recognition (CVPR), pages\\n10674–10685, 2021. 1, 2, 5, 13, 15\\n[67] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\\nnet: Convolutional networks for biomedical image segmen-\\ntation. In Medical Image Computing and Computer-Assisted\\nIntervention–MICCAI 2015: 18th International Conference,\\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\\n18, pages 234–241. Springer, 2015. 2\\n[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\\nGhasemipour, Burcu Karagol Ayan, Seyedeh Sara Mah-\\ndavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho,\\nDavid J. Fleet, and Mohammad Norouzi. Photorealistic text-\\nto-image diffusion models with deep language understand-\\ning. ArXiv, abs/2205.11487, 2022. 1, 2\\n[69] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\\nCheung, Alec Radford, and Xi Chen. Improved techniques\\nfor training gans. Advances in neural information processing\\nsystems, 29, 2016. 5\\n[70] Aditya Sanghi, Hang Chu, J. Lambourne, Ye Wang, Chin-\\nYi Cheng, and Marco Fumero. Clip-forge: Towards zero-\\nshot text-to-shape generation. 2022 IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR), pages\\n18582–18592, 2021. 2\\n[71] Fl ´avio Miguel Schneider, Zhijing Jin, and Bernhard\\nSch¨olkopf. Mo ˆusai: Text-to-music generation with long-\\ncontext latent diffusion. ArXiv, abs/2301.11757, 2023. 2\\n[72] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\\nman, et al. Laion-5b: An open large-scale dataset for train-\\ning next generation image-text models. In Thirty-sixth Con-\\nference on Neural Information Processing Systems Datasets\\nand Benchmarks Track. 2, 13\\n[73] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.\\nArXiv, abs/2111.02114, 2021. 2\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-12T02:06:26+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-12T02:06:26+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'C:\\\\Users\\\\DHRUV\\\\Desktop\\\\IProject\\\\Papers and Refrence Codes\\\\2303.11989v2.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16'}, page_content='(a) rendered depth (b) inpainted depth (c) aligned depth (d) aligned + smoothed depth\\n(e) zoom-in of (c) (f) zoom-in of (d) (g) fused mesh from (b) (h) fused mesh from (d)\\nFigure 13. Details on the depth alignment step. For each novel pose, we predict the depth for the newly generated image content (see\\nSection 3.2). First we inpaint the depth using a monocular depth prediction network (b). Then, we align inpainted depth (b) and rendered\\ndepth (a) in the least squares sense to obtain an aligned depth (c). Finally, we smooth the result to remove remaining sharp borders between\\nold and new content (d). This results in smoother, less blocky depth (e and f). Our depth alignment is necessary to create transitions without\\nholes between mesh patches (g and h).\\n(a) rendered image (b) rendered mask (c) inpaint na ¨ıve (d) dilated mask (e) inpaint dilated\\nFigure 14. Importance of mask dilation during completion. In our second stage, we complete the scene mesh by filling in unobserved\\nregions (see Section 3.4). First, we sample camera poses that view such unobserved regions (a). The unobserved regions can have arbitrary\\nsize (b). Directly inpainting only the masked regions from (b) gives distorted results, because the holes can be too small for reasonable\\ninpainting results (c). Instead, we inpaint small holes with a classical inpainting method [84] and dilate remaining holes to a larger size (d).\\nThe resulting image after inpainting contains more reasonable structure (e).\\nFigure 15. Left: DreamFusion-Inward. Mid/Right: DreamFusion-\\nOutward from in- and out-of-distribution viewpoints.\\nRGB panoramas. We then extract the mesh similarly.\\nGAUDI [5]: Bautista and Guo et al. [5] present a method\\nto generate large-scale 3D scenes encoded into a NeRF [48]\\nrepresentation. Their generative model can be conditioned\\nto produce 3D indoor scenes from text as input. In gen-\\neral, each scene allows for a different distribution of camera\\nposes. Walls and objects are placed at different positions\\nin each scene, thus it depends on the scene to determine\\nvalid camera poses. They model this joint latent distribution\\nof scenes and cameras. This allows to synthesize scenes\\nthat can be rendered from corresponding camera trajecto-\\nries (e.g., a scene is rendered in a forward motion). How-\\n16')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke(\"What is this paper bout\",k =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb77673",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"content\" : itemgetter(\"question\") | retriver, \"question\" : itemgetter(\"question\")} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636197c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The PDF appears to be a research paper titled \"Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation\" by authors Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. The paper discusses a method for generating realistic 3D scenes from text descriptions and synthesizing videos from these generated scenes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input(\"Enter your query : \")\n",
    "chain.invoke({\"question\" : question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
