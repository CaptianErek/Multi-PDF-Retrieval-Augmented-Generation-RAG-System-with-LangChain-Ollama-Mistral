# Multi-PDF-Retrieval-Augmented-Generation-RAG-System-with-LangChain-Ollama-Mistral

## Overview  

This repository contains a **Retrieval-Augmented Generation (RAG) pipeline** that allows users to **query multiple PDF documents** and receive accurate, context-aware responses generated by **Mistral** through **Ollama**. The system leverages **LangChain** for orchestration, document parsing, and retrieval, making it easy to interact with large volumes of textual data spread across multiple files.  

Imagine dropping in your research papers, technical reports, or e-books and instantly having an AI assistant capable of answering questions with references directly from your documents â€” this project brings that to life.  

## Features  

- ğŸ“‚ **Multi-PDF Support**: Load and query multiple PDF files simultaneously  
- ğŸ” **Efficient Retrieval**: Vector-based semantic search ensures relevant context extraction  
- ğŸ§  **Powered by Mistral**: Lightweight, fast, and effective responses generated with **Ollama**  
- âš¡ **LangChain Orchestration**: Streamlined pipeline for document ingestion, chunking, embedding, and retrieval  
- ğŸ§© **Extensible**: Easily adapt to other LLMs or vector stores  
- ğŸ¯ **Practical Use Cases**: Research assistance, legal/technical document analysis, knowledge management  

## Requirements  

Install the dependencies using:  

```bash
pip install langchain pypdf faiss-cpu sentence-transformers
```
You will also need to have Ollama installed and Mistral pulled locally:
```bash
curl -fsSL https://ollama.com/install.sh | sh

ollama pull mistral
```
## Libraries & Tools Used
langchain â€“ Framework for building the RAG pipeline

pypdf â€“ PDF parsing and text extraction

faiss-cpu â€“ Vector database for efficient semantic search

ollama â€“ Local LLM runtime for running Mistral

mistral â€“ LLM used for generating answers from retrieved context
