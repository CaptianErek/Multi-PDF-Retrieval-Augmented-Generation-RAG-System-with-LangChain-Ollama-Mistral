# Multi-PDF-Retrieval-Augmented-Generation-RAG-System-with-LangChain-Ollama-Mistral

## Overview  

This repository contains a **Retrieval-Augmented Generation (RAG) pipeline** that allows users to **query multiple PDF documents** and receive accurate, context-aware responses generated by **Mistral** through **Ollama**. The system leverages **LangChain** for orchestration, document parsing, and retrieval, making it easy to interact with large volumes of textual data spread across multiple files.  

Imagine dropping in your research papers, technical reports, or e-books and instantly having an AI assistant capable of answering questions with references directly from your documents — this project brings that to life.  

## Features  

- 📂 **Multi-PDF Support**: Load and query multiple PDF files simultaneously  
- 🔍 **Efficient Retrieval**: Vector-based semantic search ensures relevant context extraction  
- 🧠 **Powered by Mistral**: Lightweight, fast, and effective responses generated with **Ollama**  
- ⚡ **LangChain Orchestration**: Streamlined pipeline for document ingestion, chunking, embedding, and retrieval  
- 🧩 **Extensible**: Easily adapt to other LLMs or vector stores  
- 🎯 **Practical Use Cases**: Research assistance, legal/technical document analysis, knowledge management  

## Requirements  

Install the dependencies using:  

```bash
pip install langchain pypdf faiss-cpu sentence-transformers
```
You will also need to have Ollama installed and Mistral pulled locally:
```bash
curl -fsSL https://ollama.com/install.sh | sh

ollama pull mistral
```
## Libraries & Tools Used
langchain – Framework for building the RAG pipeline

pypdf – PDF parsing and text extraction

faiss-cpu – Vector database for efficient semantic search

ollama – Local LLM runtime for running Mistral

mistral – LLM used for generating answers from retrieved context
